{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scripts/processor.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import json\n",
    "import argparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb237956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ExoplanetPCAProcessor:\n",
    "    def __init__(self, n_components=3):\n",
    "        self.n_components = n_components\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.feature_names = [\n",
    "            'period', 'duration', 'depth', 'stellar_radius', \n",
    "            'stellar_temp', 'impact_parameter'\n",
    "        ]\n",
    "        self.fitted = False\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"Load exoplanet data from CSV file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"Loaded {len(df)} records from {filepath}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"Clean and preprocess the data\"\"\"\n",
    "        # Map common column names from NASA datasets\n",
    "        column_mapping = {\n",
    "            'pl_orbper': 'period',\n",
    "            'pl_trandur': 'duration', \n",
    "            'pl_trandep': 'depth',\n",
    "            'st_rad': 'stellar_radius',\n",
    "            'st_teff': 'stellar_temp',\n",
    "            'pl_imppar': 'impact_parameter',\n",
    "            'pl_rade': 'planet_radius',\n",
    "            'disposition': 'classification',\n",
    "            'koi_disposition': 'classification'\n",
    "        }\n",
    "        \n",
    "        # Rename columns if they exist\n",
    "        df_clean = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Select relevant features\n",
    "        feature_cols = [col for col in self.feature_names if col in df_clean.columns]\n",
    "        \n",
    "        if len(feature_cols) < 3:\n",
    "            raise ValueError(\"Insufficient feature columns found in dataset\")\n",
    "        \n",
    "        # Extract features and handle missing values\n",
    "        X = df_clean[feature_cols].copy()\n",
    "        \n",
    "        # Fill missing values with median\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "        \n",
    "        # Log transform skewed features\n",
    "        log_features = ['period', 'depth']\n",
    "        for feature in log_features:\n",
    "            if feature in X.columns:\n",
    "                X[feature] = np.log10(X[feature] + 1e-10)\n",
    "        \n",
    "        # Extract labels if available\n",
    "        y = None\n",
    "        if 'classification' in df_clean.columns:\n",
    "            y = df_clean['classification'].map({\n",
    "                'CONFIRMED': 2,\n",
    "                'CANDIDATE': 1, \n",
    "                'FALSE POSITIVE': 0,\n",
    "                'Confirmed': 2,\n",
    "                'Candidate': 1,\n",
    "                'False Positive': 0\n",
    "            }).fillna(0)\n",
    "        \n",
    "        return X.values, y, feature_cols\n",
    "    \n",
    "    def fit_transform(self, X, save_path='models/'):\n",
    "        \"\"\"Fit PCA and transform data\"\"\"\n",
    "        Path(save_path).mkdir(exist_ok=True)\n",
    "        \n",
    "        # Standardize features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Fit PCA\n",
    "        X_pca = self.pca.fit_transform(X_scaled)\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "        # Save models\n",
    "        joblib.dump(self.scaler, f'{save_path}/pca_scaler.joblib')\n",
    "        joblib.dump(self.pca, f'{save_path}/pca_model.joblib')\n",
    "        \n",
    "        # Save PCA statistics\n",
    "        pca_stats = {\n",
    "            'n_components': self.n_components,\n",
    "            'explained_variance_ratio': self.pca.explained_variance_ratio_.tolist(),\n",
    "            'cumulative_variance': np.cumsum(self.pca.explained_variance_ratio_).tolist(),\n",
    "            'feature_names': self.feature_names,\n",
    "            'components': self.pca.components_.tolist()\n",
    "        }\n",
    "        \n",
    "        with open(f'{save_path}/pca_stats.json', 'w') as f:\n",
    "            json.dump(pca_stats, f, indent=2)\n",
    "        \n",
    "        print(f\"PCA Model saved. Explained variance ratio: {self.pca.explained_variance_ratio_}\")\n",
    "        print(f\"Cumulative variance explained: {np.cumsum(self.pca.explained_variance_ratio_)}\")\n",
    "        \n",
    "        return X_pca\n",
    "    \n",
    "    def transform_new_data(self, X):\n",
    "        \"\"\"Transform new data using fitted PCA\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"PCA model not fitted. Call fit_transform first.\")\n",
    "        \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.pca.transform(X_scaled)\n",
    "    \n",
    "    def export_for_js(self, output_path='data/'):\n",
    "        \"\"\"Export PCA parameters for JavaScript usage\"\"\"\n",
    "        Path(output_path).mkdir(exist_ok=True)\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"PCA model not fitted\")\n",
    "        \n",
    "        js_export = {\n",
    "            'pca_components': self.pca.components_.tolist(),\n",
    "            'explained_variance_ratio': self.pca.explained_variance_ratio_.tolist(),\n",
    "            'mean': self.scaler.mean_.tolist(),\n",
    "            'scale': self.scaler.scale_.tolist(),\n",
    "            'n_components': self.n_components,\n",
    "            'feature_names': self.feature_names\n",
    "        }\n",
    "        \n",
    "        with open(f'{output_path}/pca_params.json', 'w') as f:\n",
    "            json.dump(js_export, f, indent=2)\n",
    "        \n",
    "        print(f\"PCA parameters exported to {output_path}/pca_params.json\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Process exoplanet data with PCA')\n",
    "    parser.add_argument('--input', '-i', required=True, help='Input CSV file path')\n",
    "    parser.add_argument('--output', '-o', default='data/', help='Output directory')\n",
    "    parser.add_argument('--components', '-c', type=int, default=3, help='Number of PCA components')\n",
    "    parser.add_argument('--test-size', '-t', type=float, default=0.2, help='Test set size')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = ExoplanetPCAProcessor(n_components=args.components)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = processor.load_data(args.input)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    X, y, feature_names = processor.preprocess_data(df)\n",
    "    print(f\"Preprocessed data shape: {X.shape}\")\n",
    "    print(f\"Features: {feature_names}\")\n",
    "    \n",
    "    # Split data\n",
    "    if y is not None:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=args.test_size, random_state=42, stratify=y\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(X, test_size=args.test_size, random_state=42)\n",
    "        y_train = y_test = None\n",
    "    \n",
    "    # Fit PCA\n",
    "    X_train_pca = processor.fit_transform(X_train)\n",
    "    X_test_pca = processor.transform_new_data(X_test)\n",
    "    \n",
    "    # Save processed data\n",
    "    Path(args.output).mkdir(exist_ok=True)\n",
    "    \n",
    "    np.save(f'{args.output}/X_train_pca.npy', X_train_pca)\n",
    "    np.save(f'{args.output}/X_test_pca.npy', X_test_pca)\n",
    "    \n",
    "    if y_train is not None:\n",
    "        np.save(f'{args.output}/y_train.npy', y_train)\n",
    "        np.save(f'{args.output}/y_test.npy', y_test)\n",
    "    \n",
    "    # Export for JavaScript\n",
    "    processor.export_for_js(args.output)\n",
    "    \n",
    "    print(f\"Processing complete. Output saved to {args.output}\")\n",
    "    print(f\"PCA Components shape: {X_train_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e8153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3992 records from ../dataset/Full Columned K2.csv\n"
     ]
    }
   ],
   "source": [
    "Processor = ExoplanetPCAProcessor(10)\n",
    "\n",
    "data = Processor.load_data(\"../dataset/Full Columned K2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7c3459c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowid</th>\n",
       "      <th>pl_name</th>\n",
       "      <th>hostname</th>\n",
       "      <th>pl_letter</th>\n",
       "      <th>k2_name</th>\n",
       "      <th>epic_hostname</th>\n",
       "      <th>epic_candname</th>\n",
       "      <th>hd_name</th>\n",
       "      <th>hip_name</th>\n",
       "      <th>tic_id</th>\n",
       "      <th>...</th>\n",
       "      <th>releasedate</th>\n",
       "      <th>pl_nnotes</th>\n",
       "      <th>k2_campaigns</th>\n",
       "      <th>k2_campaigns_num</th>\n",
       "      <th>st_nphot</th>\n",
       "      <th>st_nrvc</th>\n",
       "      <th>st_nspec</th>\n",
       "      <th>pl_nespec</th>\n",
       "      <th>pl_ntranspec</th>\n",
       "      <th>pl_ndispec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BD+20 594 b</td>\n",
       "      <td>BD+20 594</td>\n",
       "      <td>b</td>\n",
       "      <td>K2-56 b</td>\n",
       "      <td>EPIC 210848071</td>\n",
       "      <td>EPIC 210848071.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIC 26123781</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-02-15</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>BD+20 594 b</td>\n",
       "      <td>BD+20 594</td>\n",
       "      <td>b</td>\n",
       "      <td>K2-56 b</td>\n",
       "      <td>EPIC 210848071</td>\n",
       "      <td>EPIC 210848071.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIC 26123781</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-07-28</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>BD+20 594 b</td>\n",
       "      <td>BD+20 594</td>\n",
       "      <td>b</td>\n",
       "      <td>K2-56 b</td>\n",
       "      <td>EPIC 210848071</td>\n",
       "      <td>EPIC 210848071.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIC 26123781</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-04-26</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>EPIC 201111557.01</td>\n",
       "      <td>EPIC 201111557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EPIC 201111557</td>\n",
       "      <td>EPIC 201111557.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIC 176942156</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-02-15</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>EPIC 201111557.01</td>\n",
       "      <td>EPIC 201111557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EPIC 201111557</td>\n",
       "      <td>EPIC 201111557.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIC 176942156</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-08-02</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>3988</td>\n",
       "      <td>WASP-85 A b</td>\n",
       "      <td>WASP-85 A</td>\n",
       "      <td>b</td>\n",
       "      <td>K2-94 b</td>\n",
       "      <td>EPIC 201862715</td>\n",
       "      <td>EPIC 201862715.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIC 380619414</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>3989</td>\n",
       "      <td>Wolf 503 b</td>\n",
       "      <td>Wolf 503</td>\n",
       "      <td>b</td>\n",
       "      <td>K2-262 b</td>\n",
       "      <td>EPIC 212779563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HIP 67285</td>\n",
       "      <td>TIC 187278212</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-09-06</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>3990</td>\n",
       "      <td>Wolf 503 b</td>\n",
       "      <td>Wolf 503</td>\n",
       "      <td>b</td>\n",
       "      <td>K2-262 b</td>\n",
       "      <td>EPIC 212779563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HIP 67285</td>\n",
       "      <td>TIC 187278212</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-05-23</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>3991</td>\n",
       "      <td>Wolf 503 b</td>\n",
       "      <td>Wolf 503</td>\n",
       "      <td>b</td>\n",
       "      <td>K2-262 b</td>\n",
       "      <td>EPIC 212779563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HIP 67285</td>\n",
       "      <td>TIC 187278212</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-08-28</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>3992</td>\n",
       "      <td>Wolf 503 b</td>\n",
       "      <td>Wolf 503</td>\n",
       "      <td>b</td>\n",
       "      <td>K2-262 b</td>\n",
       "      <td>EPIC 212779563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HIP 67285</td>\n",
       "      <td>TIC 187278212</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3992 rows Ã— 295 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rowid            pl_name        hostname pl_letter   k2_name  \\\n",
       "0         1        BD+20 594 b       BD+20 594         b   K2-56 b   \n",
       "1         2        BD+20 594 b       BD+20 594         b   K2-56 b   \n",
       "2         3        BD+20 594 b       BD+20 594         b   K2-56 b   \n",
       "3         4  EPIC 201111557.01  EPIC 201111557       NaN       NaN   \n",
       "4         5  EPIC 201111557.01  EPIC 201111557       NaN       NaN   \n",
       "...     ...                ...             ...       ...       ...   \n",
       "3987   3988        WASP-85 A b       WASP-85 A         b   K2-94 b   \n",
       "3988   3989         Wolf 503 b        Wolf 503         b  K2-262 b   \n",
       "3989   3990         Wolf 503 b        Wolf 503         b  K2-262 b   \n",
       "3990   3991         Wolf 503 b        Wolf 503         b  K2-262 b   \n",
       "3991   3992         Wolf 503 b        Wolf 503         b  K2-262 b   \n",
       "\n",
       "       epic_hostname      epic_candname hd_name   hip_name         tic_id  \\\n",
       "0     EPIC 210848071  EPIC 210848071.01     NaN        NaN   TIC 26123781   \n",
       "1     EPIC 210848071  EPIC 210848071.01     NaN        NaN   TIC 26123781   \n",
       "2     EPIC 210848071  EPIC 210848071.01     NaN        NaN   TIC 26123781   \n",
       "3     EPIC 201111557  EPIC 201111557.01     NaN        NaN  TIC 176942156   \n",
       "4     EPIC 201111557  EPIC 201111557.01     NaN        NaN  TIC 176942156   \n",
       "...              ...                ...     ...        ...            ...   \n",
       "3987  EPIC 201862715  EPIC 201862715.01     NaN        NaN  TIC 380619414   \n",
       "3988  EPIC 212779563                NaN     NaN  HIP 67285  TIC 187278212   \n",
       "3989  EPIC 212779563                NaN     NaN  HIP 67285  TIC 187278212   \n",
       "3990  EPIC 212779563                NaN     NaN  HIP 67285  TIC 187278212   \n",
       "3991  EPIC 212779563                NaN     NaN  HIP 67285  TIC 187278212   \n",
       "\n",
       "      ... releasedate  pl_nnotes k2_campaigns k2_campaigns_num  st_nphot  \\\n",
       "0     ...  2018-02-15          1            4              1.0         0   \n",
       "1     ...  2016-07-28          1            4              1.0         0   \n",
       "2     ...  2018-04-26          1            4              1.0         0   \n",
       "3     ...  2018-02-15          0           10              1.0         0   \n",
       "4     ...  2018-08-02          0           10              1.0         0   \n",
       "...   ...         ...        ...          ...              ...       ...   \n",
       "3987  ...  2019-09-05          1            1              1.0         0   \n",
       "3988  ...  2018-09-06          0           17              1.0         1   \n",
       "3989  ...  2022-05-23          0           17              1.0         1   \n",
       "3990  ...  2025-08-28          0           17              1.0         1   \n",
       "3991  ...  2023-04-17          0           17              1.0         1   \n",
       "\n",
       "      st_nrvc  st_nspec  pl_nespec pl_ntranspec  pl_ndispec  \n",
       "0           0         0          0            0           0  \n",
       "1           0         0          0            0           0  \n",
       "2           0         0          0            0           0  \n",
       "3           0         0          0            0           0  \n",
       "4           0         0          0            0           0  \n",
       "...       ...       ...        ...          ...         ...  \n",
       "3987        0         0          0            0           0  \n",
       "3988        0         0          0            0           0  \n",
       "3989        0         0          0            0           0  \n",
       "3990        0         0          0            0           0  \n",
       "3991        0         0          0            0           0  \n",
       "\n",
       "[3992 rows x 295 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c25601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
